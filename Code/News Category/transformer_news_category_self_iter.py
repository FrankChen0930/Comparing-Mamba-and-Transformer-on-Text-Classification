# -*- coding: utf-8 -*-
"""Transformer_News_Category_self_iter.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1VrsoseGCPHIiCuDLDTP21teTMFmUhUOj
"""

!pip install pynvml

from google.colab import drive
drive.mount('/content/drive')

import os
import time
import subprocess
import joblib

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import DataLoader, TensorDataset

from sklearn.model_selection import KFold, train_test_split
from sklearn.preprocessing import LabelEncoder
from sklearn.metrics import (
    accuracy_score, f1_score,
    average_precision_score, precision_score,
    recall_score, confusion_matrix
)

from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences

import psutil
from pynvml import nvmlInit, nvmlDeviceGetHandleByIndex, nvmlDeviceGetMemoryInfo

import kagglehub

def get_gpu_memory_pynvml():
    try:
        nvmlInit()
        handle = nvmlDeviceGetHandleByIndex(0)
        info = nvmlDeviceGetMemoryInfo(handle)
        return info.used / 1024**2  # MB
    except:
        return 0

def get_gpu_memory_nvidia_smi():
    result = subprocess.check_output(
        ['nvidia-smi', '--query-gpu=memory.used', '--format=csv,nounits,noheader']
    )
    return int(result.decode('utf-8').split('\n')[0])

def get_ram_memory():
    mem = psutil.virtual_memory()
    return mem.used / 1024**2  # MB

def log_resources():
    gpu_mem = get_gpu_memory_pynvml()
    ram_mem = get_ram_memory()
    return gpu_mem, ram_mem

# 全域參數
vocab_size      = 10000
max_length      = 500
batch_size      = 64
epochs          = 5

# model 參數
embedding_dim = 64
num_layers = 1
nhead = 2
ff_dim = 2048

# 儲存目錄
'''folder_name = f"epoch={epochs},batch_size={batch_size},Embedding_dim:{embedding_dim},Hidden_size:{hidden_size},without eda"
base_path   = "/content/drive/MyDrive/Project--code/imdb/Mamba_outcome"
save_dir    = os.path.join(base_path, folder_name)
os.makedirs(save_dir, exist_ok=True)
print(f"✅ 資料夾已建立：{save_dir}")'''

class TransformerClassifierMulti(nn.Module):
    def __init__(
        self,
        vocab_size: int,
        num_classes: int,
        emb_dim: int,
        num_heads: int = 8,
        ff_dim: int = 512,
        num_layers: int = 2,
        dropout: float = 0.4
    ):
        super().__init__()
        # 嵌入與標準化
        self.embedding = nn.Embedding(vocab_size, emb_dim)
        self.norm = nn.LayerNorm(emb_dim)

        # Transformer Encoder
        encoder_layer = nn.TransformerEncoderLayer(
            d_model=emb_dim,
            nhead=num_heads,
            dim_feedforward=ff_dim,
            dropout=dropout,
            batch_first=True
        )
        self.transformer = nn.TransformerEncoder(
            encoder_layer,
            num_layers=num_layers
        )

        # 分類頭
        self.fc1 = nn.Linear(emb_dim, 32)
        self.dropout = nn.Dropout(dropout)
        self.lay_norm = nn.LayerNorm(32)
        self.fc2 = nn.Linear(32, num_classes)

    def forward(self, x: torch.LongTensor) -> torch.Tensor:
        # x: (batch_size, seq_len)
        x = self.embedding(x)           # (batch_size, seq_len, emb_dim)
        x = self.norm(x)
        x = self.transformer(x)         # (batch_size, seq_len, emb_dim)

        # 全局平均池化
        x = x.mean(dim=1)               # (batch_size, emb_dim)

        # MLP
        x = F.relu(self.fc1(x))         # (batch_size, 32)
        x = self.dropout(x)
        x = self.lay_norm(x)
        logits = self.fc2(x)            # (batch_size, num_classes)
        return logits

"""資料集處理"""

# 下載並讀取 Kaggle JSON
dataset_path = kagglehub.dataset_download("rmisra/news-category-dataset")
csv_file     = os.path.join(dataset_path, "News_Category_Dataset_v3.json")
df           = pd.read_json(csv_file, lines=True)
df = df.sample(frac=1.0, random_state=42).reset_index(drop=True)
print(df.head())


'''刪掉headline 和 short_description皆空的data'''
# 原本檢查：有值即保留
mask_head = df['headline'].notna() & (df['headline'].str.strip() != '')
mask_desc = df['short_description'].notna() & (df['short_description'].str.strip() != '')

# 只刪除同時沒值的，那就保留任何一欄有值的 row
mask_keep = mask_head | mask_desc

df = df[mask_keep].reset_index(drop=True)

print(f"篩選後剩下 {len(df)} 筆；刪除 {len(mask_keep) - mask_keep.sum()} 筆完全空值的資料")

# 文本與標籤
# 1. 把 headline 和 short_description 串起來
df['text'] = df['headline'].str.strip() + ' [SEP] ' + df['short_description'].str.strip()
texts  = df['headline'].astype(str).tolist()
labels = df['category'].tolist()

# 標籤編碼
label_encoder   = LabelEncoder()
encoded_labels  = label_encoder.fit_transform(labels)
num_classes     = len(label_encoder.classes_)

# Tokenizer → sequences
tokenizer = Tokenizer(num_words=vocab_size, oov_token="<OOV>")
tokenizer.fit_on_texts(texts)
sequences = tokenizer.texts_to_sequences(texts)
padded_sequences = pad_sequences(sequences, maxlen=max_length, padding='post', truncating='post')

# 切分訓練/測試
X_train_val, X_test, y_train_val, y_test = train_test_split(
    padded_sequences, encoded_labels, test_size=0.2, random_state=42
)
# 轉成 PyTorch Tensor
X_train_val = torch.tensor(X_train_val, dtype=torch.long)
y_train_val = torch.tensor(y_train_val, dtype=torch.long)
X_test      = torch.tensor(X_test, dtype=torch.long)
y_test      = torch.tensor(y_test, dtype=torch.long)

def run_experiment(vocab_size, max_length, batch_size, embedding_dim, num_heads, ff_dim, num_layers, dropout, save_dir, epochs=10):
    class TransformerClassifierMulti(nn.Module):
        def __init__(
            self,
            vocab_size: int,
            num_classes: int,
            emb_dim: int,
            num_heads: int = num_heads,
            ff_dim: int = ff_dim,
            num_layers: int = num_layers,
            dropout: float = dropout
        ):
            super().__init__()
            # 嵌入與標準化
            self.embedding = nn.Embedding(vocab_size, emb_dim)
            self.norm = nn.LayerNorm(emb_dim)

            # Transformer Encoder
            encoder_layer = nn.TransformerEncoderLayer(
                d_model=emb_dim,
                nhead=num_heads,
                dim_feedforward=ff_dim,
                dropout=dropout,
                batch_first=True
            )
            self.transformer = nn.TransformerEncoder(
                encoder_layer,
                num_layers=num_layers
            )

            self.attn_weights = nn.Linear(emb_dim, 1)
            # 分類頭
            self.fc1 = nn.Linear(emb_dim, 32)
            self.dropout = nn.Dropout(dropout)
            self.lay_norm = nn.LayerNorm(32)
            self.fc2 = nn.Linear(32, num_classes)

        def forward(self, x: torch.LongTensor) -> torch.Tensor:
            # x: (batch_size, seq_len)
            x = self.embedding(x)           # (batch_size, seq_len, emb_dim)
            x = self.norm(x)
            x = self.transformer(x)         # (batch_size, seq_len, emb_dim)

            # 注意力加權聚合
            attn_scores = torch.softmax(self.attn_weights(x), dim=1)  # (batch_size, seq_len, 1)
            x = torch.sum(x * attn_scores, dim=1)                    # (batch_size, emb_dim)

            # MLP
            x = F.relu(self.fc1(x))         # (batch_size, 32)
            x = self.dropout(x)
            x = self.lay_norm(x)
            logits = self.fc2(x)            # (batch_size, num_classes)
            return logits

    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    kf     = KFold(n_splits=5, shuffle=True, random_state=42)
    process= psutil.Process(os.getpid())

    fold_metrics = []
    for fold, (train_idx, val_idx) in enumerate(kf.split(X_train_val), start=1):
        print(f"\n========== Fold {fold} ===========")
        model     = TransformerClassifierMulti(vocab_size, num_classes, embedding_dim, num_heads, ff_dim, num_layers).to(device)
        optimizer = torch.optim.AdamW(model.parameters(), lr=1e-3, weight_decay=1e-3)
        criterion = nn.CrossEntropyLoss()
        train_loader = DataLoader(TensorDataset(X_train_val[train_idx], y_train_val[train_idx]),
                                  batch_size=batch_size, shuffle=True,num_workers=4,pin_memory=True)
        val_loader   = DataLoader(TensorDataset(X_train_val[val_idx],   y_train_val[val_idx]),
                                  batch_size=batch_size,num_workers=4,pin_memory=True)

        train_losses, train_accs, val_accs = [], [], []
        resource_log = []
        start_time   = time.time()

        for epoch in range(1, epochs+1):
            # 訓練
            model.train()
            epoch_loss = 0
            for xb, yb in train_loader:
                xb, yb = xb.to(device), yb.to(device)
                pred   = model(xb)
                loss   = criterion(pred, yb)
                optimizer.zero_grad()
                loss.backward()
                optimizer.step()
                epoch_loss += loss.item()

            # 資源監控
            gpu_usage = get_gpu_memory_nvidia_smi()
            ram_usage = process.memory_info().rss / 1024**2
            resource_log.append((time.time() - start_time, gpu_usage, ram_usage))

            avg_loss = epoch_loss / len(train_loader)

            # 訓練 & 驗證正確率
            model.eval()
            with torch.no_grad():
                # train acc
                train_preds  = []
                train_labels = []
                for xb, yb in train_loader:
                    xb = xb.to(device)
                    probs = model(xb).cpu()
                    train_preds.extend(torch.argmax(probs, dim=1).numpy())
                    train_labels.extend(yb.numpy())
                train_acc = accuracy_score(train_labels, train_preds)
                # val acc
                val_preds, val_labels = [], []
                for xb, yb in val_loader:
                    xb = xb.to(device)
                    probs = model(xb).cpu()
                    val_preds.extend(torch.argmax(probs, dim=1).numpy())
                    val_labels.extend(yb.numpy())
                val_acc = accuracy_score(val_labels, val_preds)

            train_losses.append(avg_loss)
            train_accs.append(train_acc)
            val_accs.append(val_acc)

            print(f"Epoch {epoch}/{epochs}, Loss: {avg_loss:.4f}, Train Acc: {train_acc:.4f}, Val Acc: {val_acc:.4f}")

        # 評估
        model.eval()
        all_preds, all_probs, all_labels = [], [], []
        with torch.no_grad():
            for xb, yb in val_loader:
                xb = xb.to(device)
                probs = model(xb).cpu()
                preds = torch.argmax(probs, dim=1)
                all_probs.extend(probs.numpy())
                all_preds.extend(preds.numpy())
                all_labels.extend(yb.numpy())


        acc = accuracy_score(all_labels, all_preds)
        ap = average_precision_score(all_labels, all_probs)
        f1 = f1_score(all_labels, all_preds, average='macro')
        prec = precision_score(all_labels, all_preds, average='macro')
        recall = recall_score(all_labels, all_preds, average='macro')
        cm = confusion_matrix(all_labels, all_preds)
        time_duration = time.time()-start_time

        max_gpu_usage = max([log[1] for log in resource_log])
        max_ram_usage = max([log[2] for log in resource_log])

        fold_metrics.append([acc, f1, ap, prec, recall, time_duration, max_gpu_usage, max_ram_usage])

        print(f"Accuracy: {acc:.4f}, Precision: {prec:.4f}, Recall: {recall:.4f}, F1: {f1:.4f}, mAP: {ap:.4f}, Time: {time_duration:.2f}s")

        # 存模型
        torch.save(model.state_dict(), f"{save_dir}/transformer_fold{fold}.pt")

        # 混淆矩陣圖
        '''fig, ax = plt.subplots()
        cax = ax.imshow(cm, cmap='Blues')
        plt.title(f"Confusion Matrix - Fold {fold}")
        plt.xlabel("Predicted")
        plt.ylabel("True")
        plt.colorbar(cax)
        for i in range(cm.shape[0]):
            for j in range(cm.shape[1]):
                ax.text(j, i, str(cm[i, j]), ha='center', va='center', color='black')
        plt.savefig(f"{save_dir}/cm_fold{fold}.png")
        plt.close()'''


        fig, ax = plt.subplots()
        cax = ax.imshow(cm, cmap='Blues')
        plt.title(f"Confusion Matrix - Fold {fold}")
        plt.xlabel("Predicted")
        plt.ylabel("True")
        plt.colorbar(cax)

        # 顯示類別標籤（這是你要加的部分）
        ax.set_xticks(range(num_classes))
        ax.set_yticks(range(num_classes))
        ax.set_xticklabels(label_encoder.classes_, rotation=90)
        ax.set_yticklabels(label_encoder.classes_)

        # 顯示每格的數值
        for i in range(cm.shape[0]):
          for j in range(cm.shape[1]):
              ax.text(j, i, str(cm[i, j]), ha='center', va='center', color='black')

        plt.tight_layout()  # 避免類別名稱擠在一起
        plt.savefig(f"{save_dir}/cm_fold{fold}.png")
        plt.close()

        # Resource usage 曲線
        times, gpu_usages, ram_usages = zip(*resource_log)
        plt.figure()
        plt.plot(times, gpu_usages, label='GPU Memory (MB)')
        plt.plot(times, ram_usages, label='RAM Usage (MB)')
        plt.xlabel('Time (s)')
        plt.ylabel('Usage (MB)')
        plt.title(f'Resource Usage - Fold {fold}')
        plt.legend()
        plt.savefig(f"{save_dir}/resource_usage_fold{fold}.png")
        plt.close()

        fold += 1

    # 平均結果
    avg_metrics = np.mean(fold_metrics, axis=0)

    # ====================
    # ✅ Test 評估流程加在這裡
    # ====================
    best_model_path = f"{save_dir}/transformer_fold1.pt"  # 或你選平均後最佳 fold
    model     = TransformerClassifierMulti(vocab_size, num_classes, embedding_dim, num_heads, ff_dim, num_layers).to(device)
    model.load_state_dict(torch.load(best_model_path))
    model.eval()

    test_loader = DataLoader(TensorDataset(X_test, y_test), batch_size=batch_size)

    test_preds, test_probs, test_labels = [], [], []
    process = psutil.Process(os.getpid())
    start_time = time.time()
    ram_log = []

    with torch.no_grad():
        for xb, yb in test_loader:
            xb = xb.to(device)
            probs = model(xb).cpu()
            preds = torch.argmax(probs, dim=1)
            test_probs.extend(probs.numpy())
            test_preds.extend(preds.numpy())
            test_labels.extend(yb.numpy())
            ram_usage = process.memory_info().rss / 1024 / 1024  # MB
            ram_log.append(ram_usage)

    end_time = time.time()
    test_duration = end_time - start_time
    max_test_ram = max(ram_log)

    test_acc = accuracy_score(test_labels, test_preds)
    test_f1 = f1_score(test_labels, test_preds, average='macro')
    test_ap = average_precision_score(test_labels, test_probs, average='macro')  # 如果是 multiclass
    test_prec = precision_score(test_labels, test_preds, average='macro')
    test_recall = recall_score(test_labels, test_preds, average='macro')
    test_cm = confusion_matrix(test_labels, test_preds)

    # 存 test confusion matrix
    fig, ax = plt.subplots()
    cax = ax.imshow(test_cm, cmap='Blues')
    plt.title("Confusion Matrix - Test")
    plt.xlabel("Predicted")
    plt.ylabel("True")
    plt.colorbar(cax)
    for i in range(test_cm.shape[0]):
        for j in range(test_cm.shape[1]):
            ax.text(j, i, str(test_cm[i, j]), ha='center', va='center', color='black')
    plt.savefig(f"{save_dir}/cm_test.png")
    plt.close()

    # Model 參數量
    model_params = sum(p.numel() for p in model.parameters())

    # 寫入報告
    results_path = os.path.join(save_dir, "results_summary.txt")
    with open(results_path, "w") as f:
        f.write(f"===== Hyperparameters =====\n")
        f.write(f"Vocab Size: {vocab_size}\n")
        f.write(f"Max Length: {max_length}\n")
        f.write(f"Batch Size: {batch_size}\n")
        f.write(f"Epochs: {epochs}\n")
        f.write(f"Embedding dim: {embedding_dim}\n")
        f.write(f"Number of heads: {num_heads}\n")
        f.write(f"Feedforward dimension: {ff_dim}\n")
        f.write(f"Number of layers: {num_layers}\n")
        f.write(f"Model Params: {model_params}\n\n")

        f.write("===== Fold Metrics =====\n")
        for i, (acc, f1, ap, prec, rec, time_duration, max_gpu, max_ram) in enumerate(fold_metrics, 1):
            f.write(f"Fold {i}: Accuracy={acc:.4f}, Precision={prec:.4f}, Recall={rec:.4f}, F1={f1:.4f}, mAP={ap:.4f}, Time={time_duration:.2f}s\n")
            f.write(f"         Max GPU: {max_gpu:.2f} MB, Max RAM: {max_ram:.2f} MB\n")

        f.write(f"\n===== Average Metrics =====\n")
        f.write(f"Accuracy={avg_metrics[0]:.4f}, Precision={avg_metrics[3]:.4f}, Recall={avg_metrics[4]:.4f}, F1={avg_metrics[1]:.4f}, mAP={avg_metrics[2]:.4f}, Time={avg_metrics[5]:.2f}s\n")
        f.write(f"Max GPU: {avg_metrics[6]:.2f} MB, Max RAM: {avg_metrics[7]:.2f} MB\n")

        f.write("\n===== Test Metrics =====\n")
        f.write(f"Test Accuracy: {test_acc:.4f}, Precision: {test_prec:.4f}, Recall: {test_recall:.4f}, F1: {test_f1:.4f}, mAP: {test_ap:.4f}\n")
        f.write(f"Test Time: {test_duration:.2f}s, Max RAM Usage: {max_test_ram:.2f} MB\n")

vocab_size       = 20000
batch_size      = 256
max_length       = 250
embedding_dim    = 128
ff_dim = 512
num_heads =  4
num_layers  = 1
dropout = 0.05

folder_name = f"exp_single_vs{vocab_size}_ml{max_length}_bs{batch_size}_ed{embedding_dim}_nh{num_heads}"
base_path   = "/content/drive/MyDrive/Project--code/News/Transformer_outcome_1"
save_dir    = os.path.join(base_path, folder_name)
os.makedirs(save_dir, exist_ok=True)

run_experiment(vocab_size, max_length, batch_size, embedding_dim, num_heads, ff_dim, num_layers, dropout, save_dir, epochs=10)

# 自動化多組參數實驗
import itertools

param_grid = {
    "vocab_size":      [20000],
    "max_length":      [250],
    "batch_size":      [256],
    "embedding_dim":   [128],
    "num_heads":     [8],
    "ff_dim":      [1024, 2048],
}

param_combinations = list(itertools.product(
    param_grid["vocab_size"],
    param_grid["max_length"],
    param_grid["batch_size"],
    param_grid["embedding_dim"],
    param_grid["num_heads"],
    param_grid["ff_dim"],


))

num_layers = 1
dropout = 0.05
for i, (vs, ml, bs, ed, nh, fd) in enumerate(param_combinations, start=125):
    print(f"\n===== Running Experiment {i}/{len(param_combinations)+124} =====")
    folder_name = f"exp_{i}_vs{vs}_ml{ml}_bs{bs}_ed{ed}_hs{nh}_ks{fd}"
    base_path   = f"/content/drive/MyDrive/Project--code/News/Transformer_outcome"
    save_dir    = os.path.join(base_path, folder_name)
    os.makedirs(save_dir, exist_ok=True)
    run_experiment(vs, ml, bs, ed, nh, fd, num_layers, dropout, save_dir, epochs=10)