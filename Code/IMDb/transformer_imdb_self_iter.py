# -*- coding: utf-8 -*-
"""Transformer_imdb_self_iter.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/100wCFyiqczQU1kAZuX5QHBJ46EMzokcO
"""

!pip install pynvml
!pip install GPUtil

from google.colab import drive
drive.mount('/content/drive')

import os
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import DataLoader, TensorDataset
from sklearn.metrics import accuracy_score, f1_score, average_precision_score, confusion_matrix, precision_score, recall_score
from sklearn.model_selection import KFold
from keras.datasets import imdb
from tensorflow.keras.preprocessing.sequence import pad_sequences
import numpy as np
import matplotlib.pyplot as plt
import joblib
import time

# ========資源監控函數==========
import psutil
from pynvml import *

def get_gpu_memory():
    try:
        nvmlInit()
        handle = nvmlDeviceGetHandleByIndex(0)
        info = nvmlDeviceGetMemoryInfo(handle)
        return info.used / 1024**2  # MB
    except:
        return 0

def get_ram_memory():
    mem = psutil.virtual_memory()
    return mem.used / 1024**2  # MB

def log_resources():
    gpu_mem = get_gpu_memory()
    ram_mem = get_ram_memory()
    return gpu_mem, ram_mem

# 參數設定
vocab_size = 10000
max_length = 500
batch_size = 64
epochs = 5

#model中的參數
embedding_dim = 64
num_layers = 2
nhead = 2
ff_dim = 2048

# === 根據參數組出資料夾名稱 ===
folder_name = f"epoch={epochs},batch_size={batch_size},Embedding_dim:{emb_dim}"
base_path = "/content/drive/MyDrive/Project--code/imdb/Transformer_outcome"
save_dir = os.path.join(base_path, folder_name)

# === 建立資料夾（如果還沒存在） ===
os.makedirs(save_dir, exist_ok=True)

print(f"✅ 資料夾已建立：{save_dir}")

class TransformerClassifier(nn.Module):
    def __init__(self, vocab_size, emb_dim=embedding_dim, nhead=nhead, num_layers=num_layers, ff_dim = ff_dim):
        super().__init__()
        self.embedding = nn.Embedding(vocab_size, emb_dim)
        encoder_layer = nn.TransformerEncoderLayer(d_model=emb_dim, nhead=nhead, dim_feedforward=ff_dim, batch_first=True)
        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)
        self.fc1 = nn.Linear(emb_dim, 64)
        self.fc2 = nn.Linear(64, 1)

    def forward(self, x):
        x = self.embedding(x)
        x = self.transformer(x)
        x = x.mean(dim=1)
        x = F.relu(self.fc1(x))
        return torch.sigmoid(self.fc2(x)).squeeze(1)

# 裝置選擇
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# 資料準備
(X_train_val, y_train_val), (X_test, y_test) = imdb.load_data(num_words=vocab_size)
X_train_val = pad_sequences(X_train_val, maxlen=max_length)
X_test = pad_sequences(X_test, maxlen=max_length)
X_train_val = torch.tensor(X_train_val, dtype=torch.long)
y_train_val = torch.tensor(y_train_val, dtype=torch.float32)
X_test = torch.tensor(X_test, dtype=torch.long)
y_test = torch.tensor(y_test, dtype=torch.float32)

import subprocess

num_layers =1
def get_gpu_memory():
    result = subprocess.check_output(
        ['nvidia-smi', '--query-gpu=memory.used', '--format=csv,nounits,noheader']
    )
    return int(result.decode('utf-8').split('\n')[0])

def run_experiment(vocab_size, max_length, batch_size, embedding_dim, nhead, ff_dim, save_dir):
    class TransformerClassifier(nn.Module):
        def __init__(self, vocab_size, emb_dim=embedding_dim, nhead=nhead, num_layers=num_layers, ff_dim = ff_dim):
            super().__init__()
            self.embedding = nn.Embedding(vocab_size, emb_dim)
            encoder_layer = nn.TransformerEncoderLayer(d_model=emb_dim, nhead=nhead, dim_feedforward=ff_dim, batch_first=True)
            self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)
            self.fc1 = nn.Linear(emb_dim, 64)
            self.fc2 = nn.Linear(64, 1)

        def forward(self, x):
            x = self.embedding(x)
            x = self.transformer(x)
            x = x.mean(dim=1)
            x = F.relu(self.fc1(x))
            return torch.sigmoid(self.fc2(x)).squeeze(1)


    kf = KFold(n_splits=5, shuffle=True, random_state=42)
    process = psutil.Process(os.getpid())

    fold_metrics = []
    fold = 1

    for train_idx, val_idx in kf.split(X_train_val):
        print(f"\n========== Fold {fold} ===========")
        model = TransformerClassifier(vocab_size).to(device)
        optimizer = torch.optim.AdamW(model.parameters(), lr=5e-3, weight_decay=1e-3)
        criterion = nn.BCELoss()

        train_loader = DataLoader(TensorDataset(X_train_val[train_idx], y_train_val[train_idx]), batch_size=batch_size, shuffle=True)
        val_loader = DataLoader(TensorDataset(X_train_val[val_idx], y_train_val[val_idx]), batch_size=batch_size)

        train_losses, train_accuracies, val_accuracies = [], [], []
        resource_log = []
        start_time = time.time()

        for epoch in range(epochs):
            model.train()
            epoch_loss = 0
            for xb, yb in train_loader:
                xb, yb = xb.to(device), yb.to(device)
                pred = model(xb)
                loss = criterion(pred, yb)
                optimizer.zero_grad()
                loss.backward()
                optimizer.step()
                epoch_loss += loss.item()

            # Resource monitoring
            gpu_usage = get_gpu_memory()
            ram_usage = process.memory_info().rss / 1024 / 1024
            resource_log.append((time.time() - start_time, gpu_usage, ram_usage))

            avg_loss = epoch_loss / len(train_loader)

            model.eval()
            train_preds, train_labels_eval = [], []
            with torch.no_grad():
                for xb, yb in train_loader:
                    xb = xb.to(device)
                    probs = model(xb).cpu()
                    preds = (probs > 0.5).int()
                    train_preds.extend(preds.numpy())
                    train_labels_eval.extend(yb.numpy())
            train_acc = accuracy_score(train_labels_eval, train_preds)
            train_accuracies.append(train_acc)
            train_losses.append(avg_loss)

            val_preds, val_labels = [], []
            with torch.no_grad():
                for xb, yb in val_loader:
                    xb = xb.to(device)
                    probs = model(xb).cpu()
                    preds = (probs > 0.5).int()
                    val_preds.extend(preds.numpy())
                    val_labels.extend(yb.numpy())
            val_acc = accuracy_score(val_labels, val_preds)
            val_accuracies.append(val_acc)

            print(f"Epoch {epoch+1}/{epochs}, Loss: {avg_loss:.4f}, Train Acc: {train_acc:.4f}, Val Acc: {val_acc:.4f}")

        # 評估
        model.eval()
        all_preds, all_probs, all_labels = [], [], []
        with torch.no_grad():
            for xb, yb in val_loader:
                xb = xb.to(device)
                probs = model(xb).cpu()
                preds = (probs > 0.5).int()
                all_probs.extend(probs.numpy())
                all_preds.extend(preds.numpy())
                all_labels.extend(yb.numpy())

        acc = accuracy_score(all_labels, all_preds)
        f1 = f1_score(all_labels, all_preds)
        ap = average_precision_score(all_labels, all_probs)
        prec = precision_score(all_labels, all_preds)
        recall = recall_score(all_labels, all_preds)
        cm = confusion_matrix(all_labels, all_preds)
        time_duration = time.time()-start_time

        max_gpu_usage = max([log[1] for log in resource_log])
        max_ram_usage = max([log[2] for log in resource_log])

        fold_metrics.append([acc, f1, ap, prec, recall, time_duration, max_gpu_usage, max_ram_usage])

        print(f"Accuracy: {acc:.4f}, Precision: {prec:.4f}, Recall: {recall:.4f}, F1: {f1:.4f}, mAP: {ap:.4f}, Time: {time_duration:.2f}s")

        # 存模型
        torch.save(model.state_dict(), f"{save_dir}/transformer_fold{fold}.pt")

        # 混淆矩陣圖
        fig, ax = plt.subplots()
        cax = ax.imshow(cm, cmap='Blues')
        plt.title(f"Confusion Matrix - Fold {fold}")
        plt.xlabel("Predicted")
        plt.ylabel("True")
        plt.colorbar(cax)
        for i in range(cm.shape[0]):
            for j in range(cm.shape[1]):
                ax.text(j, i, str(cm[i, j]), ha='center', va='center', color='black')
        plt.savefig(f"{save_dir}/cm_fold{fold}.png")
        plt.close()

        # Resource usage 曲線
        times, gpu_usages, ram_usages = zip(*resource_log)
        plt.figure()
        plt.plot(times, gpu_usages, label='GPU Memory (MB)')
        plt.plot(times, ram_usages, label='RAM Usage (MB)')
        plt.xlabel('Time (s)')
        plt.ylabel('Usage (MB)')
        plt.title(f'Resource Usage - Fold {fold}')
        plt.legend()
        plt.savefig(f"{save_dir}/resource_usage_fold{fold}.png")
        plt.close()

        fold += 1

    # 平均結果
    avg_metrics = np.mean(fold_metrics, axis=0)

    # ====================
    # ✅ Test 評估流程加在這裡
    # ====================
    best_model_path = f"{save_dir}/transformer_fold1.pt"  # 或你選平均後最佳 fold
    model = TransformerClassifier(vocab_size).to(device)
    model.load_state_dict(torch.load(best_model_path))
    model.eval()

    test_loader = DataLoader(TensorDataset(X_test, y_test), batch_size=batch_size)

    test_preds, test_probs, test_labels = [], [], []
    process = psutil.Process(os.getpid())
    start_time = time.time()
    ram_log = []

    with torch.no_grad():
        for xb, yb in test_loader:
            xb = xb.to(device)
            probs = model(xb).cpu()
            preds = (probs > 0.5).int()
            test_probs.extend(probs.numpy())
            test_preds.extend(preds.numpy())
            test_labels.extend(yb.numpy())
            ram_usage = process.memory_info().rss / 1024 / 1024  # MB
            ram_log.append(ram_usage)

    end_time = time.time()
    test_duration = end_time - start_time
    max_test_ram = max(ram_log)

    test_acc = accuracy_score(all_labels, all_preds)
    test_f1 = f1_score(all_labels, all_preds)
    test_ap = average_precision_score(all_labels, all_probs)
    test_prec = precision_score(all_labels, all_preds)
    test_recall = recall_score(all_labels, all_preds)
    test_cm = confusion_matrix(all_labels, all_preds)

    # 存 test confusion matrix
    fig, ax = plt.subplots()
    cax = ax.imshow(test_cm, cmap='Blues')
    plt.title("Confusion Matrix - Test")
    plt.xlabel("Predicted")
    plt.ylabel("True")
    plt.colorbar(cax)
    for i in range(test_cm.shape[0]):
        for j in range(test_cm.shape[1]):
            ax.text(j, i, str(test_cm[i, j]), ha='center', va='center', color='black')
    plt.savefig(f"{save_dir}/cm_test.png")
    plt.close()

    # Model 參數量
    model_params = sum(p.numel() for p in model.parameters())

    # 寫入報告
    results_path = os.path.join(save_dir, "results_summary.txt")
    with open(results_path, "w") as f:
        f.write(f"===== Hyperparameters =====\n")
        f.write(f"Vocab Size: {vocab_size}\n")
        f.write(f"Max Length: {max_length}\n")
        f.write(f"Batch Size: {batch_size}\n")
        f.write(f"Epochs: {epochs}\n")
        f.write(f"Embedding dim: {embedding_dim}\n")
        f.write(f"nhead: {nhead}\n")
        f.write(f"ff_dim: {ff_dim}\n")
        f.write(f"Model Params: {model_params}\n\n")

        f.write("===== Fold Metrics =====\n")
        for i, (acc, f1, ap, prec, rec, time_duration, max_gpu, max_ram) in enumerate(fold_metrics, 1):
            f.write(f"Fold {i}: Accuracy={acc:.4f}, Precision={prec:.4f}, Recall={rec:.4f}, F1={f1:.4f}, mAP={ap:.4f}, Time={time_duration:.2f}s\n")
            f.write(f"         Max GPU: {max_gpu:.2f} MB, Max RAM: {max_ram:.2f} MB\n")

        f.write(f"\n===== Average Metrics =====\n")
        f.write(f"Accuracy={avg_metrics[0]:.4f}, Precision={avg_metrics[3]:.4f}, Recall={avg_metrics[4]:.4f}, F1={avg_metrics[1]:.4f}, mAP={avg_metrics[2]:.4f}, Time={avg_metrics[5]:.2f}s\n")
        f.write(f"Max GPU: {avg_metrics[6]:.2f} MB, Max RAM: {avg_metrics[7]:.2f} MB\n")

        f.write("\n===== Test Metrics =====\n")
        f.write(f"Test Accuracy: {test_acc:.4f}, Precision: {test_prec:.4f}, Recall: {test_recall:.4f}, F1: {test_f1:.4f}, mAP: {test_ap:.4f}\n")
        f.write(f"Test Time: {test_duration:.2f}s, Max RAM Usage: {max_test_ram:.2f} MB\n")

import itertools

# 設定參數組合
param_grid = {
    "vocab_size": [10000, 20000],
    "max_length": [200, 500],
    "batch_size": [32, 64],
    "embedding_dim": [32, 64, 128],
    "nhead": [2, 4],
    "ff_dim": [256, 512, 1024] #default is 2048

}

# 產生所有參數組合
param_combinations = list(itertools.product(
    param_grid["vocab_size"],
    param_grid["max_length"],
    param_grid["batch_size"],
    param_grid["embedding_dim"],
    param_grid["nhead"],
    param_grid["ff_dim"]
))

# 自動實驗跑起來
for i, (vocab_size, max_length, batch_size, embedding_dim, nhead, ff_dim) in enumerate(param_combinations, start=1):
    print(f"\n===== Running Experiment {i}/{len(param_combinations)} =====")
    folder_name = f"exp_{i}_vs{vocab_size}_ml{max_length}_bs{batch_size}_ed{embedding_dim}_nh{nhead}_ffd{ff_dim}"
    base_path = "/content/drive/MyDrive/Project--code/imdb/Transformer_outcome_2"
    save_dir = os.path.join(base_path, folder_name)
    os.makedirs(save_dir, exist_ok=True)
    run_experiment(vocab_size, max_length, batch_size, embedding_dim, nhead, ff_dim, save_dir)

from google.colab import runtime
runtime.unassign()